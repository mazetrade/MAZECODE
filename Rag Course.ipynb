{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de fichiers PDF trouvés : 13\n",
      "- BASF_sustainability_report_2024.pdf\n",
      "- Bayer_sustainability_report_2024.pdf\n",
      "- Cargill_sustainability_report_2024.pdf\n",
      "- CK_Hutchinson _sustainability_report_2024.pdf\n",
      "- Honeywell_sustainability_report_2024.pdf\n",
      "- Merck_sustainability_report_2024.pdf\n",
      "- Microsoft_sustainability_report_2024.pdf\n",
      "- Qualcomm_sustainability_report_2023.pdf\n",
      "- Shell_sustainability_report_2023.pdf\n",
      "- Thai_Oil_sustainability_report_2023.pdf\n",
      "- Veolia_sustainability_report_2024.pdf\n",
      "- Verizon_sustainability_report_2023.pdf\n",
      "- Walmart_sustainability_report_2023.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Chemin vers votre dossier contenant les PDF\n",
    "pdf_folder = r\"C:\\Users\\Mazen ISMAIL\\Downloads\\Rag project\\Data\"\n",
    "\n",
    "# Récupérer tous les fichiers PDF dans le dossier\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "print(f\"Nombre de fichiers PDF trouvés : {len(pdf_files)}\")\n",
    "\n",
    "# Afficher les noms des fichiers trouvés\n",
    "for pdf in pdf_files:\n",
    "    print(f\"- {os.path.basename(pdf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement de BASF_sustainability_report_2024.pdf...\n",
      "  - 2064 chunks créés\n",
      "Traitement de Bayer_sustainability_report_2024.pdf...\n",
      "Erreur avec Bayer_sustainability_report_2024.pdf: PyCryptodome is required for AES algorithm\n",
      "Traitement de Cargill_sustainability_report_2024.pdf...\n",
      "  - 968 chunks créés\n",
      "Traitement de CK_Hutchinson _sustainability_report_2024.pdf...\n",
      "  - 1119 chunks créés\n",
      "Traitement de Honeywell_sustainability_report_2024.pdf...\n",
      "  - 621 chunks créés\n",
      "Traitement de Merck_sustainability_report_2024.pdf...\n",
      "  - 1465 chunks créés\n",
      "Traitement de Microsoft_sustainability_report_2024.pdf...\n",
      "  - 1115 chunks créés\n",
      "Traitement de Qualcomm_sustainability_report_2023.pdf...\n",
      "  - 831 chunks créés\n",
      "Traitement de Shell_sustainability_report_2023.pdf...\n",
      "  - 1177 chunks créés\n",
      "Traitement de Thai_Oil_sustainability_report_2023.pdf...\n",
      "  - 803 chunks créés\n",
      "Traitement de Veolia_sustainability_report_2024.pdf...\n",
      "  - 392 chunks créés\n",
      "Traitement de Verizon_sustainability_report_2023.pdf...\n",
      "  - 924 chunks créés\n",
      "Traitement de Walmart_sustainability_report_2023.pdf...\n",
      "  - 285 chunks créés\n",
      "Total : 11764 chunks créés\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.schema import Document\n",
    "import PyPDF2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Initialiser le text splitter pour un chunking plus fin et hiérarchique\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=350,  # Taille réduite comme demandé\n",
    "    chunk_overlap=50,  # Chevauchement adapté à la nouvelle taille\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Séparateurs plus précis\n",
    ")\n",
    "\n",
    "pdf_folder = r\"C:\\Users\\Mazen ISMAIL\\Downloads\\Rag project\\Data\"\n",
    "pdf_files = glob.glob(os.path.join(pdf_folder, \"*.pdf\"))\n",
    "\n",
    "# Charger et découper les PDF un par un avec support des tableaux et images\n",
    "all_chunks = []\n",
    "for pdf_file in pdf_files:\n",
    "    try:\n",
    "        print(f\"Traitement de {os.path.basename(pdf_file)}...\")\n",
    "        \n",
    "        # Utiliser PyPDF2 pour extraire le texte\n",
    "        with open(pdf_file, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            pdf_text = \"\"\n",
    "            \n",
    "            # Métadonnées du document\n",
    "            doc_metadata = {\n",
    "                \"source\": pdf_file,\n",
    "                \"title\": os.path.basename(pdf_file),\n",
    "                \"total_pages\": len(reader.pages)\n",
    "            }\n",
    "            \n",
    "            for page_num, page in enumerate(reader.pages):\n",
    "                # Extraire le texte\n",
    "                text = page.extract_text() or \"\"\n",
    "                \n",
    "                # Combiner avec des marqueurs de structure\n",
    "                page_content = f\"# Page {page_num+1}\\n{text}\"\n",
    "                pdf_text += page_content\n",
    "        \n",
    "        # Découper avec préservation de la structure\n",
    "        try:\n",
    "            header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "                (\"#\", \"header1\"),\n",
    "                (\"##\", \"header2\"),\n",
    "            ])\n",
    "            md_header_splits = header_splitter.split_text(pdf_text)\n",
    "            \n",
    "            # Découper en chunks tout en préservant les métadonnées hiérarchiques\n",
    "            chunks = []\n",
    "            for split in md_header_splits:\n",
    "                # Préserver le contexte hiérarchique dans les métadonnées\n",
    "                split_metadata = {**doc_metadata, **split.metadata}\n",
    "                # Découper davantage si nécessaire\n",
    "                smaller_chunks = text_splitter.split_text(split.page_content)\n",
    "                # Créer des Documents avec contexte préservé\n",
    "                for chunk in smaller_chunks:\n",
    "                    chunks.append(Document(page_content=chunk, metadata=split_metadata))\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du découpage hiérarchique: {e}\")\n",
    "            # Fallback au découpage simple\n",
    "            chunks = []\n",
    "            with open(pdf_file, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                for i, page in enumerate(reader.pages):\n",
    "                    text = page.extract_text() or \"\"\n",
    "                    page_doc = Document(\n",
    "                        page_content=text,\n",
    "                        metadata={\"source\": pdf_file, \"page\": i+1}\n",
    "                    )\n",
    "                    page_chunks = text_splitter.split_documents([page_doc])\n",
    "                    chunks.extend(page_chunks)\n",
    "        \n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"  - {len(chunks)} chunks créés\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {os.path.basename(pdf_file)}: {e}\")\n",
    "\n",
    "print(f\"Total : {len(all_chunks)} chunks créés\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding créé avec succès - Dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"OpenAIKey.env\")\n",
    "\n",
    "# Créer l'instance d'embeddings\n",
    "try:\n",
    "    # Pour GPT-4o mini, utilisez le modèle d'embedding approprié \n",
    "    # (text-embedding-3-small est un bon choix économique)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    # Test pour vérifier que ça fonctionne\n",
    "    if all_chunks:\n",
    "        test_text = all_chunks[0].page_content[:100]\n",
    "        test_embedding = embeddings.embed_query(test_text)\n",
    "        print(f\"✅ Embedding créé avec succès - Dimension: {len(test_embedding)}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erreur lors de la création des embeddings: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création des embeddings pour tous les chunks...\n",
      "Traitement du lot 1/236...\n",
      "Traitement du lot 2/236...\n",
      "Traitement du lot 3/236...\n",
      "Traitement du lot 4/236...\n",
      "Traitement du lot 5/236...\n",
      "Traitement du lot 6/236...\n",
      "Traitement du lot 7/236...\n",
      "Traitement du lot 8/236...\n",
      "Traitement du lot 9/236...\n",
      "Traitement du lot 10/236...\n",
      "Traitement du lot 11/236...\n",
      "Traitement du lot 12/236...\n",
      "Traitement du lot 13/236...\n",
      "Traitement du lot 14/236...\n",
      "Traitement du lot 15/236...\n",
      "Traitement du lot 16/236...\n",
      "Traitement du lot 17/236...\n",
      "Traitement du lot 18/236...\n",
      "Traitement du lot 19/236...\n",
      "Traitement du lot 20/236...\n",
      "Traitement du lot 21/236...\n",
      "Traitement du lot 22/236...\n",
      "Traitement du lot 23/236...\n",
      "Traitement du lot 24/236...\n",
      "Traitement du lot 25/236...\n",
      "Traitement du lot 26/236...\n",
      "Traitement du lot 27/236...\n",
      "Traitement du lot 28/236...\n",
      "Traitement du lot 29/236...\n",
      "Traitement du lot 30/236...\n",
      "Traitement du lot 31/236...\n",
      "Traitement du lot 32/236...\n",
      "Traitement du lot 33/236...\n",
      "Traitement du lot 34/236...\n",
      "Traitement du lot 35/236...\n",
      "Traitement du lot 36/236...\n",
      "Traitement du lot 37/236...\n",
      "Traitement du lot 38/236...\n",
      "Traitement du lot 39/236...\n",
      "Traitement du lot 40/236...\n",
      "Traitement du lot 41/236...\n",
      "Traitement du lot 42/236...\n",
      "Traitement du lot 43/236...\n",
      "Traitement du lot 44/236...\n",
      "Traitement du lot 45/236...\n",
      "Traitement du lot 46/236...\n",
      "Traitement du lot 47/236...\n",
      "Traitement du lot 48/236...\n",
      "Traitement du lot 49/236...\n",
      "Traitement du lot 50/236...\n",
      "Traitement du lot 51/236...\n",
      "Traitement du lot 52/236...\n",
      "Traitement du lot 53/236...\n",
      "Traitement du lot 54/236...\n",
      "Traitement du lot 55/236...\n",
      "Traitement du lot 56/236...\n",
      "Traitement du lot 57/236...\n",
      "Traitement du lot 58/236...\n",
      "Traitement du lot 59/236...\n",
      "Traitement du lot 60/236...\n",
      "Traitement du lot 61/236...\n",
      "Traitement du lot 62/236...\n",
      "Traitement du lot 63/236...\n",
      "Traitement du lot 64/236...\n",
      "Traitement du lot 65/236...\n",
      "Traitement du lot 66/236...\n",
      "Traitement du lot 67/236...\n",
      "Traitement du lot 68/236...\n",
      "Traitement du lot 69/236...\n",
      "Traitement du lot 70/236...\n",
      "Traitement du lot 71/236...\n",
      "Traitement du lot 72/236...\n",
      "Traitement du lot 73/236...\n",
      "Traitement du lot 74/236...\n",
      "Traitement du lot 75/236...\n",
      "Traitement du lot 76/236...\n",
      "Traitement du lot 77/236...\n",
      "Traitement du lot 78/236...\n",
      "Traitement du lot 79/236...\n",
      "Traitement du lot 80/236...\n",
      "Traitement du lot 81/236...\n",
      "Traitement du lot 82/236...\n",
      "Traitement du lot 83/236...\n",
      "Traitement du lot 84/236...\n",
      "Traitement du lot 85/236...\n",
      "Traitement du lot 86/236...\n",
      "Traitement du lot 87/236...\n",
      "Traitement du lot 88/236...\n",
      "Traitement du lot 89/236...\n",
      "Traitement du lot 90/236...\n",
      "Traitement du lot 91/236...\n",
      "Traitement du lot 92/236...\n",
      "Traitement du lot 93/236...\n",
      "Traitement du lot 94/236...\n",
      "Traitement du lot 95/236...\n",
      "Traitement du lot 96/236...\n",
      "Traitement du lot 97/236...\n",
      "Traitement du lot 98/236...\n",
      "Traitement du lot 99/236...\n",
      "Traitement du lot 100/236...\n",
      "Traitement du lot 101/236...\n",
      "Traitement du lot 102/236...\n",
      "Traitement du lot 103/236...\n",
      "Traitement du lot 104/236...\n",
      "Traitement du lot 105/236...\n",
      "Traitement du lot 106/236...\n",
      "Traitement du lot 107/236...\n",
      "Traitement du lot 108/236...\n",
      "Traitement du lot 109/236...\n",
      "Traitement du lot 110/236...\n",
      "Traitement du lot 111/236...\n",
      "Traitement du lot 112/236...\n",
      "Traitement du lot 113/236...\n",
      "Traitement du lot 114/236...\n",
      "Traitement du lot 115/236...\n",
      "Traitement du lot 116/236...\n",
      "Traitement du lot 117/236...\n",
      "Traitement du lot 118/236...\n",
      "Traitement du lot 119/236...\n",
      "Traitement du lot 120/236...\n",
      "Traitement du lot 121/236...\n",
      "Traitement du lot 122/236...\n",
      "Traitement du lot 123/236...\n",
      "Traitement du lot 124/236...\n",
      "Traitement du lot 125/236...\n",
      "Traitement du lot 126/236...\n",
      "Traitement du lot 127/236...\n",
      "Traitement du lot 128/236...\n",
      "Traitement du lot 129/236...\n",
      "Traitement du lot 130/236...\n",
      "Traitement du lot 131/236...\n",
      "Traitement du lot 132/236...\n",
      "Traitement du lot 133/236...\n",
      "Traitement du lot 134/236...\n",
      "Traitement du lot 135/236...\n",
      "Traitement du lot 136/236...\n",
      "Traitement du lot 137/236...\n",
      "Traitement du lot 138/236...\n",
      "Traitement du lot 139/236...\n",
      "Traitement du lot 140/236...\n",
      "Traitement du lot 141/236...\n",
      "Traitement du lot 142/236...\n",
      "Traitement du lot 143/236...\n",
      "Traitement du lot 144/236...\n",
      "Traitement du lot 145/236...\n",
      "Traitement du lot 146/236...\n",
      "Traitement du lot 147/236...\n",
      "Traitement du lot 148/236...\n",
      "Traitement du lot 149/236...\n",
      "Traitement du lot 150/236...\n",
      "Traitement du lot 151/236...\n",
      "Traitement du lot 152/236...\n",
      "Traitement du lot 153/236...\n",
      "Traitement du lot 154/236...\n",
      "Traitement du lot 155/236...\n",
      "Traitement du lot 156/236...\n",
      "Traitement du lot 157/236...\n",
      "Traitement du lot 158/236...\n",
      "Traitement du lot 159/236...\n",
      "Traitement du lot 160/236...\n",
      "Traitement du lot 161/236...\n",
      "Traitement du lot 162/236...\n",
      "Traitement du lot 163/236...\n",
      "Traitement du lot 164/236...\n",
      "Traitement du lot 165/236...\n",
      "Traitement du lot 166/236...\n",
      "Traitement du lot 167/236...\n",
      "Traitement du lot 168/236...\n",
      "Traitement du lot 169/236...\n",
      "Traitement du lot 170/236...\n",
      "Traitement du lot 171/236...\n",
      "Traitement du lot 172/236...\n",
      "Traitement du lot 173/236...\n",
      "Traitement du lot 174/236...\n",
      "Traitement du lot 175/236...\n",
      "Traitement du lot 176/236...\n",
      "Traitement du lot 177/236...\n",
      "Traitement du lot 178/236...\n",
      "Traitement du lot 179/236...\n",
      "Traitement du lot 180/236...\n",
      "Traitement du lot 181/236...\n",
      "Traitement du lot 182/236...\n",
      "Traitement du lot 183/236...\n",
      "Traitement du lot 184/236...\n",
      "Traitement du lot 185/236...\n",
      "Traitement du lot 186/236...\n",
      "Traitement du lot 187/236...\n",
      "Traitement du lot 188/236...\n",
      "Traitement du lot 189/236...\n",
      "Traitement du lot 190/236...\n",
      "Traitement du lot 191/236...\n",
      "Traitement du lot 192/236...\n",
      "Traitement du lot 193/236...\n",
      "Traitement du lot 194/236...\n",
      "Traitement du lot 195/236...\n",
      "Traitement du lot 196/236...\n",
      "Traitement du lot 197/236...\n",
      "Traitement du lot 198/236...\n",
      "Traitement du lot 199/236...\n",
      "Traitement du lot 200/236...\n",
      "Traitement du lot 201/236...\n",
      "Traitement du lot 202/236...\n",
      "Traitement du lot 203/236...\n",
      "Traitement du lot 204/236...\n",
      "Traitement du lot 205/236...\n",
      "Traitement du lot 206/236...\n",
      "Traitement du lot 207/236...\n",
      "Traitement du lot 208/236...\n",
      "Traitement du lot 209/236...\n",
      "Traitement du lot 210/236...\n",
      "Traitement du lot 211/236...\n",
      "Traitement du lot 212/236...\n",
      "Traitement du lot 213/236...\n",
      "Traitement du lot 214/236...\n",
      "Traitement du lot 215/236...\n",
      "Traitement du lot 216/236...\n",
      "Traitement du lot 217/236...\n",
      "Traitement du lot 218/236...\n",
      "Traitement du lot 219/236...\n",
      "Traitement du lot 220/236...\n",
      "Traitement du lot 221/236...\n",
      "Traitement du lot 222/236...\n",
      "Traitement du lot 223/236...\n",
      "Traitement du lot 224/236...\n",
      "Traitement du lot 225/236...\n",
      "Traitement du lot 226/236...\n",
      "Traitement du lot 227/236...\n",
      "Traitement du lot 228/236...\n",
      "Traitement du lot 229/236...\n",
      "Traitement du lot 230/236...\n",
      "Traitement du lot 231/236...\n",
      "Traitement du lot 232/236...\n",
      "Traitement du lot 233/236...\n",
      "Traitement du lot 234/236...\n",
      "Traitement du lot 235/236...\n",
      "Traitement du lot 236/236...\n",
      "✅ Embeddings créés pour 11764 chunks sur 11764\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour traiter les chunks par lots\n",
    "def process_chunks_in_batches(chunks, batch_size=100):\n",
    "    \"\"\"Traite les chunks par lots pour éviter les limitations d'API\"\"\"\n",
    "    total_chunks = len(chunks)\n",
    "    results = []\n",
    "    \n",
    "    for i in range(0, total_chunks, batch_size):\n",
    "        batch = chunks[i:min(i + batch_size, total_chunks)]\n",
    "        print(f\"Traitement du lot {i//batch_size + 1}/{(total_chunks-1)//batch_size + 1}...\")\n",
    "        \n",
    "        # Extraire le texte de chaque chunk pour l'embedding\n",
    "        texts = [chunk.page_content for chunk in batch]\n",
    "        \n",
    "        try:\n",
    "            # Créer les embeddings pour ce lot\n",
    "            batch_embeddings = embeddings.embed_documents(texts)\n",
    "            \n",
    "            # Stocker les résultats (texte, embedding, métadonnées)\n",
    "            for j, embedding_vector in enumerate(batch_embeddings):\n",
    "                chunk_data = {\n",
    "                    \"text\": batch[j].page_content,\n",
    "                    \"embedding\": embedding_vector,\n",
    "                    \"metadata\": batch[j].metadata\n",
    "                }\n",
    "                results.append(chunk_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du lot {i//batch_size + 1}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Traiter tous les chunks\n",
    "print(\"Création des embeddings pour tous les chunks...\")\n",
    "embedded_chunks = process_chunks_in_batches(all_chunks, batch_size=50)\n",
    "print(f\"✅ Embeddings créés pour {len(embedded_chunks)} chunks sur {len(all_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Aucun index FAISS existant trouvé.\n",
      "Création de l'index FAISS...\n",
      "Sauvegarde de l'index FAISS dans C:\\Users\\Mazen ISMAIL\\Downloads\\Rag project\\faiss_index...\n",
      "✅ Index FAISS créé et sauvegardé avec succès!\n",
      "Test de recherche pour: 'Qu'est-ce que le RAG?'\n",
      "Trouvé 2 résultats pertinents.\n",
      "Exemple de résultat: procedure (e-Rapid Incident Report). The aim is to identify risks at an early stage and, if necessary,\n",
      "initiate appropriate remedial and communication...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Créer un répertoire pour stocker l'index FAISS si nécessaire\n",
    "index_folder = os.path.join(os.path.dirname(pdf_folder), \"faiss_index\")\n",
    "os.makedirs(index_folder, exist_ok=True)\n",
    "\n",
    "# Chemin du fichier de sauvegarde\n",
    "index_file_path = os.path.join(index_folder, \"pdf_embeddings_index.faiss\")\n",
    "index_pkl_path = os.path.join(index_folder, \"pdf_embeddings_index.pkl\")\n",
    "\n",
    "# Fonction pour créer et sauvegarder l'index FAISS\n",
    "def create_and_save_faiss_index(embedded_chunks, embeddings_model):\n",
    "    # Préparer les données pour FAISS\n",
    "    texts = [item[\"text\"] for item in embedded_chunks]\n",
    "    embeddings_list = [item[\"embedding\"] for item in embedded_chunks]\n",
    "    metadatas = [item[\"metadata\"] for item in embedded_chunks]\n",
    "    \n",
    "    # Créer l'index FAISS\n",
    "    print(\"Création de l'index FAISS...\")\n",
    "    vector_store = FAISS.from_embeddings(\n",
    "        text_embeddings=list(zip(texts, embeddings_list)),\n",
    "        embedding=embeddings_model,\n",
    "        metadatas=metadatas\n",
    "    )\n",
    "    \n",
    "    # Sauvegarder l'index\n",
    "    print(f\"Sauvegarde de l'index FAISS dans {index_folder}...\")\n",
    "    vector_store.save_local(index_folder)\n",
    "    \n",
    "    print(\"✅ Index FAISS créé et sauvegardé avec succès!\")\n",
    "    return vector_store\n",
    "\n",
    "# Fonction pour charger l'index FAISS s'il existe déjà\n",
    "def load_faiss_index(embeddings_model):\n",
    "    if os.path.exists(os.path.join(index_folder, \"index.faiss\")):\n",
    "        print(\"Chargement de l'index FAISS existant...\")\n",
    "        try:\n",
    "            vector_store = FAISS.load_local(index_folder, embeddings_model)\n",
    "            print(\"✅ Index FAISS chargé avec succès!\")\n",
    "            return vector_store\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de l'index: {e}\")\n",
    "    \n",
    "    print(\"❌ Aucun index FAISS existant trouvé.\")\n",
    "    return None\n",
    "\n",
    "# Créer ou charger l'index FAISS\n",
    "vector_store = load_faiss_index(embeddings)\n",
    "if vector_store is None:\n",
    "    vector_store = create_and_save_faiss_index(embedded_chunks, embeddings)\n",
    "\n",
    "# Vérifier que l'index fonctionne\n",
    "test_query = \"Qu'est-ce que le RAG?\"\n",
    "print(f\"Test de recherche pour: '{test_query}'\")\n",
    "test_results = vector_store.similarity_search(test_query, k=2)\n",
    "print(f\"Trouvé {len(test_results)} résultats pertinents.\")\n",
    "if test_results:\n",
    "    print(f\"Exemple de résultat: {test_results[0].page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction modifiée pour charger l'index FAISS avec l'option de sécurité activée\n",
    "def load_faiss_index(embeddings_model):\n",
    "    if os.path.exists(os.path.join(index_folder, \"index.faiss\")):\n",
    "        print(\"Chargement de l'index FAISS existant...\")\n",
    "        try:\n",
    "            vector_store = FAISS.load_local(\n",
    "                index_folder, \n",
    "                embeddings_model,\n",
    "                allow_dangerous_deserialization=True  # Ajout de cette option\n",
    "            )\n",
    "            print(\"✅ Index FAISS chargé avec succès!\")\n",
    "            return vector_store\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement de l'index: {e}\")\n",
    "    \n",
    "    print(\"❌ Aucun index FAISS existant trouvé.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:53:11.057 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.schema import Document\n",
    "import fitz  # PyMuPDF\n",
    "import tempfile\n",
    "import shutil\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Chargement des variables d'environnement\n",
    "load_dotenv(dotenv_path=\"OpenAIKey.env\")\n",
    "\n",
    "# Configuration de la page Streamlit avec thème\n",
    "st.set_page_config(\n",
    "    page_title=\"Assistant RAG pour PDF\",\n",
    "    page_icon=\"📚\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:53:16.738 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:17.028 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run c:\\Users\\Mazen ISMAIL\\env\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-18 20:53:17.030 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Appliquer des styles CSS améliorés\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    /* Styles pour les titres et sous-titres */\n",
    "    .main-header {\n",
    "        font-size: 2.5rem;\n",
    "        font-weight: 700;\n",
    "        color: #1E88E5;\n",
    "        margin-bottom: 1.5rem;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    }\n",
    "    .sub-header {\n",
    "        font-size: 1.8rem;\n",
    "        font-weight: 600;\n",
    "        color: #0D47A1;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        margin-top: 1.5rem;\n",
    "        margin-bottom: 1rem;\n",
    "    }\n",
    "    \n",
    "    /* Box d'information */\n",
    "    .info-box {\n",
    "        background-color: #E3F2FD;\n",
    "        padding: 1.2rem;\n",
    "        border-radius: 0.7rem;\n",
    "        margin-bottom: 1.5rem;\n",
    "        border-left: 5px solid #1E88E5;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    }\n",
    "    \n",
    "    /* Conteneur de résultat */\n",
    "    .result-container {\n",
    "        background-color: #FAFAFA;\n",
    "        padding: 1.8rem;\n",
    "        border-radius: 0.7rem;\n",
    "        border-left: 7px solid #1E88E5;\n",
    "        margin-top: 1.5rem;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        line-height: 1.6;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);\n",
    "    }\n",
    "    \n",
    "    /* Titre de source */\n",
    "    .source-title {\n",
    "        font-weight: 600;\n",
    "        color: #0D47A1;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    }\n",
    "    \n",
    "    /* Section de barre latérale */\n",
    "    .sidebar-section {\n",
    "        background-color: #F5F5F5;\n",
    "        padding: 1.2rem;\n",
    "        border-radius: 0.7rem;\n",
    "        margin-bottom: 1.5rem;\n",
    "        border-left: 4px solid #1E88E5;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    }\n",
    "    \n",
    "    /* Style pour les tabs */\n",
    "    .stTabs [data-baseweb=\"tab-list\"] {\n",
    "        gap: 2rem;\n",
    "    }\n",
    "    \n",
    "    .stTabs [data-baseweb=\"tab\"] {\n",
    "        height: 3rem;\n",
    "        white-space: pre-wrap;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "    \n",
    "    /* Tableaux de données */\n",
    "    .dataframe-container {\n",
    "        border-radius: 0.7rem;\n",
    "        overflow: hidden;\n",
    "        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.05);\n",
    "        margin-top: 1rem;\n",
    "    }\n",
    "    \n",
    "    /* Boutons */\n",
    "    .stButton > button {\n",
    "        font-weight: 600;\n",
    "        padding: 0.6rem 1.2rem;\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "    }\n",
    "    \n",
    "    /* Uniformisation des polices pour tout le contenu */\n",
    "    body, .stTextInput label, .stTextInput input, .stTextArea label, .stTextArea textarea,\n",
    "    .stSelectbox label, .stSelectbox select, .stSlider label, p, div, span {\n",
    "        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif !important;\n",
    "    }\n",
    "    \n",
    "    /* Style pour les métriques */\n",
    "    .metric-container {\n",
    "        background-color: #F5F5F5;\n",
    "        padding: 1rem;\n",
    "        border-radius: 0.5rem;\n",
    "        text-align: center;\n",
    "        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);\n",
    "    }\n",
    "    \n",
    "    .metric-value {\n",
    "        font-size: 1.8rem;\n",
    "        font-weight: 700;\n",
    "        color: #1E88E5;\n",
    "    }\n",
    "    \n",
    "    .metric-label {\n",
    "        font-size: 1rem;\n",
    "        color: #555;\n",
    "        margin-top: 0.3rem;\n",
    "    }\n",
    "    \n",
    "    /* Style pour les onglets principaux */\n",
    "    .main-tabs {\n",
    "        margin-top: 1.5rem;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:06:45.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:06:45.753 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:06:45.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:06:45.755 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:06:45.758 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:06:45.758 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:06:45.758 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Chemin vers l'index FAISS préexistant\n",
    "index_folder = r\"C:\\Users\\Mazen ISMAIL\\Downloads\\Rag project\\faiss_index\"\n",
    "\n",
    "# Titre et description\n",
    "st.markdown('<div class=\"main-header\">📚 Assistant RAG pour PDF</div>', unsafe_allow_html=True)\n",
    "st.markdown(\"\"\"\n",
    "<div class=\"info-box\">\n",
    "Cet assistant utilise la technologie <b>RAG (Retrieval Augmented Generation)</b> pour répondre à vos questions\n",
    "sur les documents PDF. Grâce à l'extraction sémantique avancée, il peut:\n",
    "<ul>\n",
    "    <li>Préserver la structure des documents, y compris les tableaux</li>\n",
    "    <li>Agréger l'information provenant de plusieurs sources</li>\n",
    "    <li>Filtrer les résultats non pertinents grâce à la similarité cosine</li>\n",
    "</ul>\n",
    "</div>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "# Création des onglets principaux\n",
    "tab1, tab2 = st.tabs([\"🔍 **Recherche**\", \"📤 **Gestion des documents**\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:53:31.299 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:31.301 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:31.303 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:31.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:31.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:31.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:31.806 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Fonction pour charger l'index FAISS existant avec cache\n",
    "@st.cache_resource\n",
    "def load_vector_store():\n",
    "    try:\n",
    "        embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "        vector_store = FAISS.load_local(\n",
    "            index_folder, \n",
    "            embeddings,\n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        return vector_store, True\n",
    "    except Exception as e:\n",
    "        st.error(f\"Erreur lors du chargement de l'index FAISS: {e}\")\n",
    "        return None, False\n",
    "\n",
    "# Charger l'index au démarrage\n",
    "vector_store, success = load_vector_store()\n",
    "\n",
    "# Fonction pour obtenir les sources disponibles\n",
    "def get_all_sources(vector_store):\n",
    "    try:\n",
    "        all_docs = vector_store.docstore._dict.values()\n",
    "        sources = set()\n",
    "        for doc in all_docs:\n",
    "            source = os.path.basename(doc.metadata.get('source', ''))\n",
    "            if source:\n",
    "                sources.add(source)\n",
    "        return ['Tous les documents'] + sorted(list(sources))\n",
    "    except:\n",
    "        return ['Tous les documents']\n",
    "\n",
    "# Liste étendue des modèles OpenAI\n",
    "openai_models = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"gpt-4.1\",\n",
    "    \"o4-mini\",          # tout nouveau, successeur d’o3-mini :contentReference[oaicite:5]{index=5}\n",
    "    \"o3\",               # grand modèle raisonnement :contentReference[oaicite:6]{index=6}\n",
    "    \"o3-mini\",\n",
    "    \"text-embedding-3-small\",\n",
    "    \"text-embedding-3-large\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 20:53:36.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.510 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.514 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.516 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.517 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.519 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.522 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.524 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.530 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.533 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.535 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.538 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.539 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.540 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-18 20:53:36.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.540 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.542 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.544 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.546 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.547 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.550 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.552 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.553 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.554 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.555 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.557 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.559 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.562 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.563 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.565 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.567 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.569 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.569 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.571 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.573 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.573 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.575 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.576 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.576 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.577 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.577 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.577 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.579 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.581 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.583 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.585 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.586 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.587 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.588 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.589 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.590 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 20:53:36.591 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Onglet 1: Recherche\n",
    "with tab1:\n",
    "    # Paramètres dans la barre latérale\n",
    "    with st.sidebar:\n",
    "        st.markdown('<div class=\"sidebar-section\">', unsafe_allow_html=True)\n",
    "        st.header(\"⚙️ Paramètres\")\n",
    "\n",
    "        # Nombre de chunks et seuil de similarité\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            k_value = st.slider(\"Chunks à récupérer\", 1, 15, 5)\n",
    "        with col2:\n",
    "            similarity_threshold = st.slider(\"Seuil similarité\", 0.0, 1.0, 0.7, 0.05)\n",
    "\n",
    "        # Température du modèle\n",
    "        temperature = st.slider(\"Température du LLM\", 0.0, 1.0, 0.1, 0.1, \n",
    "                              help=\"Contrôle la créativité du modèle. Plus élevé = plus créatif mais moins précis.\")\n",
    "\n",
    "        # Choix du modèle (liste étendue)\n",
    "        llm_model = st.selectbox(\n",
    "            \"Modèle LLM\",\n",
    "            openai_models,\n",
    "            index=0,\n",
    "            help=\"Sélectionnez le modèle OpenAI à utiliser pour la génération de réponses.\"\n",
    "        )\n",
    "\n",
    "        # Option pour l'agrégation\n",
    "        enable_aggregation = st.checkbox(\"Activer l'agrégation avancée\", True,\n",
    "                                      help=\"Utilise map_reduce pour synthétiser l'information de plusieurs sources\")\n",
    "        st.markdown('</div>', unsafe_allow_html=True)\n",
    "\n",
    "        # Filtrage par sources si l'index est chargé\n",
    "        if success:\n",
    "            st.markdown('<div class=\"sidebar-section\">', unsafe_allow_html=True)\n",
    "            st.header(\"🔍 Filtrage\")\n",
    "            all_sources = get_all_sources(vector_store)\n",
    "            selected_sources = st.multiselect(\n",
    "                \"Filtrer par documents\",\n",
    "                options=all_sources,\n",
    "                default=[\"Tous les documents\"]\n",
    "            )\n",
    "            st.markdown('</div>', unsafe_allow_html=True)\n",
    "        else:\n",
    "            selected_sources = [\"Tous les documents\"]\n",
    "\n",
    "    # Interface utilisateur du chat\n",
    "    st.markdown('<div class=\"sub-header\">💬 Posez votre question</div>', unsafe_allow_html=True)\n",
    "\n",
    "    # Zone de saisie pour la question\n",
    "    query = st.text_area(\"Votre question sur les documents:\", height=100)\n",
    "\n",
    "    # Ajouter des exemples de questions\n",
    "    with st.expander(\"Exemples de questions\"):\n",
    "        example_queries = [\n",
    "            \"Qu'est-ce que le RAG?\",\n",
    "            \"Quels sont les avantages des systèmes RAG par rapport aux modèles classiques?\",\n",
    "            \"Comment fonctionnent les embeddings dans un système RAG?\",\n",
    "            \"Quelles sont les sources d'énergies renouvelables mentionnées dans le document?\",\n",
    "            \"Quel est le plan d'investissement de TotalEnergies pour 2023-2025?\"\n",
    "        ]\n",
    "        cols = st.columns(3)\n",
    "        for i, example in enumerate(example_queries):\n",
    "            if cols[i % 3].button(example, key=f\"example_{i}\"):\n",
    "                query = example\n",
    "\n",
    "    # Option pour afficher la visualisation\n",
    "    show_visualization = st.checkbox(\"Afficher la visualisation des chunks\", False)\n",
    "\n",
    "    # Bouton pour soumettre la requête\n",
    "    submit_button = st.button(\"🔍 Rechercher\", type=\"primary\", use_container_width=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour interroger le RAG avec similarité cosine améliorée\n",
    "def query_rag(query):\n",
    "    # Créer le LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=llm_model,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    \n",
    "    # Créer le retriever avec seuil de similarité\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity_score_threshold\",\n",
    "        search_kwargs={\n",
    "            \"k\": k_value,\n",
    "            \"score_threshold\": similarity_threshold\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Filtrer par source si spécifié\n",
    "    if selected_sources and 'Tous les documents' not in selected_sources:\n",
    "        # Créer une fonction de filtrage pour les sources sélectionnées\n",
    "        def filter_by_sources(docs):\n",
    "            return [\n",
    "                doc for doc in docs \n",
    "                if os.path.basename(doc.metadata.get('source', '')) in selected_sources\n",
    "            ]\n",
    "        \n",
    "        # Appliquer le filtre au retriever\n",
    "        retriever.search_kwargs[\"filter_fn\"] = filter_by_sources\n",
    "    \n",
    "    # Template de prompt amélioré pour la chaîne stuff\n",
    "    prompt_template = \"\"\"\n",
    "    Tu es un assistant d'information spécialisé qui aide à répondre aux questions basées sur des documents PDF.\n",
    "    \n",
    "    Utilise UNIQUEMENT les informations fournies dans le contexte ci-dessous pour répondre.\n",
    "    Si l'information n'est pas présente dans le contexte, dis simplement \"Je ne trouve pas cette information dans les documents fournis.\"\n",
    "    Ne fabrique pas de réponse.\n",
    "    \n",
    "    IMPORTANT: \n",
    "    - Synthétise les informations provenant de sources multiples si nécessaire.\n",
    "    - Si des informations contradictoires apparaissent, signale-le et présente les différentes perspectives.\n",
    "    - Pour les informations numériques ou factuelles, cite précisément les sources avec le numéro de page.\n",
    "    - Si des tableaux sont mentionnés dans les chunks, présente les données sous forme structurée.\n",
    "    - Mentionne toujours le numéro de page exact dans tes citations.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXTE:\n",
    "    {context}\n",
    "    \n",
    "    RÉPONSE (synthétise les informations de manière cohérente, avec citations des sources et numéros de page):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Templates spécifiques pour map-reduce\n",
    "    map_template = \"\"\"\n",
    "    Tu es un assistant d'information spécialisé qui aide à répondre aux questions basées sur des documents PDF.\n",
    "    \n",
    "    Résume le contexte suivant pour répondre à la question.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    CONTEXTE:\n",
    "    {context}\n",
    "    \n",
    "    RÉSUMÉ (ne réponds pas encore à la question, fais uniquement un résumé des informations pertinentes):\n",
    "    \"\"\"\n",
    "    \n",
    "    reduce_template = \"\"\"\n",
    "    Tu es un assistant d'information spécialisé qui aide à répondre aux questions basées sur des documents PDF.\n",
    "    \n",
    "    Utilise UNIQUEMENT les informations fournies dans les résumés ci-dessous pour répondre.\n",
    "    Si l'information n'est pas présente dans les résumés, dis simplement \"Je ne trouve pas cette information dans les documents fournis.\"\n",
    "    Ne fabrique pas de réponse.\n",
    "    \n",
    "    IMPORTANT: \n",
    "    - Synthétise les informations provenant de sources multiples si nécessaire.\n",
    "    - Si des informations contradictoires apparaissent, signale-le et présente les différentes perspectives.\n",
    "    - Pour les informations numériques ou factuelles, cite précisément les sources, si mentionnées dans les résumés.\n",
    "    - Si des tableaux sont mentionnés, présente les données sous forme structurée.\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    RÉSUMÉS:\n",
    "    {summaries}\n",
    "    \n",
    "    RÉPONSE (synthétise les informations de manière cohérente, avec citations des sources si disponibles):\n",
    "    \"\"\"\n",
    "    \n",
    "    STUFF_PROMPT = PromptTemplate(\n",
    "        template=prompt_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    MAP_PROMPT = PromptTemplate(\n",
    "        template=map_template,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    REDUCE_PROMPT = PromptTemplate(\n",
    "        template=reduce_template,\n",
    "        input_variables=[\"summaries\", \"question\"]\n",
    "    )\n",
    "    \n",
    "    # Créer la chaîne RAG avec capacité d'agrégation\n",
    "    if enable_aggregation:\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"map_reduce\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\n",
    "                \"question_prompt\": MAP_PROMPT,\n",
    "                \"combine_prompt\": REDUCE_PROMPT\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=True,\n",
    "            chain_type_kwargs={\"prompt\": STUFF_PROMPT}\n",
    "        )\n",
    "    \n",
    "    # Exécuter la requête\n",
    "    result = qa_chain({\"query\": query})\n",
    "    \n",
    "    return {\n",
    "        \"answer\": result[\"result\"],\n",
    "        \"source_docs\": result[\"source_documents\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traiter la requête si une question est posée et le bouton est cliqué\n",
    "if query and submit_button:\n",
    "    if success:\n",
    "        with st.status(\"🔍 Recherche d'informations dans les documents...\") as status:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                # Exécuter la requête\n",
    "                result = query_rag(query)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                status.update(label=\"✅ Recherche terminée!\", state=\"complete\")\n",
    "            except Exception as e:\n",
    "                st.error(f\"Erreur lors de la recherche: {str(e)}\")\n",
    "                import traceback\n",
    "                st.error(traceback.format_exc())\n",
    "                st.stop()\n",
    "            \n",
    "            # Afficher la réponse\n",
    "            st.markdown('<div class=\"sub-header\">📝 Réponse</div>', unsafe_allow_html=True)\n",
    "            st.markdown(f'<div class=\"result-container\">{result[\"answer\"]}</div>', unsafe_allow_html=True)\n",
    "            \n",
    "            # Afficher des métriques\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                st.markdown('<div class=\"metric-container\">', unsafe_allow_html=True)\n",
    "                st.markdown(f'<div class=\"metric-value\">{elapsed_time:.2f}s</div>', unsafe_allow_html=True)\n",
    "                st.markdown('<div class=\"metric-label\">Temps de réponse</div>', unsafe_allow_html=True)\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "                \n",
    "            with col2:\n",
    "                st.markdown('<div class=\"metric-container\">', unsafe_allow_html=True)\n",
    "                st.markdown(f'<div class=\"metric-value\">{len(set([os.path.basename(doc.metadata.get(\"source\", \"\")) for doc in result[\"source_docs\"]]))}</div>', unsafe_allow_html=True)\n",
    "                st.markdown('<div class=\"metric-label\">Documents utilisés</div>', unsafe_allow_html=True)\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "                \n",
    "            with col3:\n",
    "                st.markdown('<div class=\"metric-container\">', unsafe_allow_html=True)\n",
    "                st.markdown(f'<div class=\"metric-value\">{len(result[\"source_docs\"])}</div>', unsafe_allow_html=True)\n",
    "                st.markdown('<div class=\"metric-label\">Chunks utilisés</div>', unsafe_allow_html=True)\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "            \n",
    "            # Afficher les sources\n",
    "            st.markdown('<div class=\"sub-header\">📄 Sources utilisées</div>', unsafe_allow_html=True)\n",
    "            source_docs = result[\"source_docs\"]\n",
    "            \n",
    "            if source_docs:\n",
    "                sources_data = []\n",
    "                for i, doc in enumerate(source_docs):\n",
    "                    source_name = os.path.basename(doc.metadata.get('source', 'Inconnu'))\n",
    "                    page_num = None\n",
    "                    import re\n",
    "                    if 'page' in doc.metadata:\n",
    "                        page_num = doc.metadata['page']\n",
    "                    elif 'header1' in doc.metadata and 'Page ' in doc.metadata['header1']:\n",
    "                        page_num = doc.metadata['header1'].replace('Page ', '')\n",
    "                    elif 'header1' in doc.metadata:\n",
    "                        match = re.search(r'Page (\\d+)', doc.metadata['header1'])\n",
    "                        if match:\n",
    "                            page_num = match.group(1)\n",
    "                    if page_num is None:\n",
    "                        match = re.search(r'# Page (\\d+)', doc.page_content)\n",
    "                        page_num = match.group(1) if match else 'N/A'\n",
    "                    \n",
    "                    context = doc.metadata.get('header1', '') + ' > ' + doc.metadata.get('header2', '')\n",
    "                    similarity = doc.metadata.get('score', 'N/A') if 'score' in doc.metadata else 'N/A'\n",
    "                    \n",
    "                    sources_data.append({\n",
    "                        \"N°\": i+1,\n",
    "                        \"Document\": source_name,\n",
    "                        \"Page\": page_num,\n",
    "                        \"Contexte\": context,\n",
    "                        \"Pertinence\": similarity if isinstance(similarity, float) else 'N/A',\n",
    "                        \"Extrait\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content\n",
    "                    })\n",
    "                \n",
    "                st.markdown('<div class=\"dataframe-container\">', unsafe_allow_html=True)\n",
    "                st.dataframe(\n",
    "                    pd.DataFrame(sources_data),\n",
    "                    use_container_width=True,\n",
    "                    column_config={\n",
    "                        \"N°\": st.column_config.NumberColumn(\"N°\", width=\"small\"),\n",
    "                        \"Document\": st.column_config.TextColumn(\"Document\", width=\"medium\"),\n",
    "                        \"Page\": st.column_config.TextColumn(\"Page\", width=\"small\"),\n",
    "                        \"Contexte\": st.column_config.TextColumn(\"Contexte\", width=\"medium\"),\n",
    "                        \"Extrait\": st.column_config.TextColumn(\"Extrait\", width=\"large\"),\n",
    "                        \"Pertinence\": st.column_config.ProgressColumn(\n",
    "                            \"Pertinence\", \n",
    "                            min_value=0, \n",
    "                            max_value=1,\n",
    "                            format=\"%.2f\"\n",
    "                        ) if any(isinstance(x.get(\"Pertinence\"), float) for x in sources_data) else None\n",
    "                    }\n",
    "                )\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "                \n",
    "                if show_visualization and len(source_docs) > 1:\n",
    "                    try:\n",
    "                        st.markdown('<div class=\"sub-header\">📊 Visualisation des chunks</div>', unsafe_allow_html=True)\n",
    "                        viz_data = pd.DataFrame(sources_data)\n",
    "                        st.subheader(\"Distribution des chunks par document\")\n",
    "                        doc_counts = viz_data[\"Document\"].value_counts()\n",
    "                        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "                        doc_counts.plot(kind='pie', autopct='%1.1f%%', ax=ax)\n",
    "                        plt.ylabel('')\n",
    "                        st.pyplot(fig)\n",
    "                        \n",
    "                        pertinence_cols = [col for col in viz_data.columns if 'Pertinence' in col]\n",
    "                        if pertinence_cols and viz_data[pertinence_cols[0]].apply(lambda x: isinstance(x, (int, float))).any():\n",
    "                            st.subheader(\"Pertinence des chunks récupérés\")\n",
    "                            fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                            sns.barplot(x=\"N°\", y=pertinence_cols[0], data=viz_data, palette=\"viridis\", ax=ax)\n",
    "                            ax.set_title(\"Score de similarité par chunk\")\n",
    "                            ax.set_ylabel(\"Score de similarité\")\n",
    "                            ax.set_xlabel(\"Numéro du chunk\")\n",
    "                            plt.tight_layout()\n",
    "                            st.pyplot(fig)\n",
    "                    except Exception as e:\n",
    "                        st.info(f\"Impossible de générer la visualisation. Erreur: {str(e)}\")\n",
    "            else:\n",
    "                st.info(\"Aucune source pertinente trouvée. Essayez d'ajuster le seuil de similarité ou les paramètres de filtrage.\")\n",
    "            \n",
    "            st.markdown(\"### Évaluation de la réponse\")\n",
    "            feedback = st.radio(\n",
    "                \"Cette réponse était-elle utile?\",\n",
    "                [\"Très utile\", \"Partiellement utile\", \"Pas utile\"]\n",
    "            )\n",
    "            feedback_text = st.text_area(\"Commentaires supplémentaires (facultatif):\", height=100, key=\"feedback\")\n",
    "            if st.button(\"Envoyer l'évaluation\"):\n",
    "                st.success(\"Merci pour votre feedback! Il nous aidera à améliorer le système.\")\n",
    "    else:\n",
    "        st.error(\"Impossible de charger l'index FAISS. Veuillez vérifier que l'index existe et est accessible.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:19:17.142 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.145 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.147 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.149 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.153 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.155 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.157 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.159 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.160 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.162 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.164 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.166 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.166 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.168 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.170 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.172 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.174 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:19:17.176 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Onglet 2: Gestion des documents\n",
    "with tab2:\n",
    "    st.markdown('<div class=\"sub-header\">📤 Ajouter de nouveaux documents</div>', unsafe_allow_html=True)\n",
    "    \n",
    "    # Section améliorée pour l'upload de fichiers\n",
    "    st.markdown(\"\"\"\n",
    "    <div class=\"info-box\">\n",
    "    Cette section vous permet d'ajouter de nouveaux documents PDF à l'index. Les fichiers seront automatiquement traités,\n",
    "    découpés en chunks et indexés pour être disponibles dans vos recherches.\n",
    "    </div>\n",
    "    \"\"\", unsafe_allow_html=True)\n",
    "    \n",
    "    # Zone d'upload bien visible\n",
    "    uploaded_files = st.file_uploader(\n",
    "        \"Téléchargez vos fichiers PDF\",\n",
    "        type=['pdf'],\n",
    "        accept_multiple_files=True,\n",
    "        help=\"Vous pouvez télécharger plusieurs fichiers PDF simultanément\"\n",
    "    )\n",
    "    \n",
    "    # Paramètres de traitement des PDF\n",
    "    with st.expander(\"Paramètres avancés de traitement\"):\n",
    "        col1, col2 = st.columns(2)\n",
    "        with col1:\n",
    "            chunk_size = st.slider(\"Taille des chunks\", 200, 1000, 350, 50,\n",
    "                                help=\"Plus petite taille = plus de précision, plus grande taille = plus de contexte\")\n",
    "        with col2:\n",
    "            chunk_overlap = st.slider(\"Chevauchement\", 0, 200, 50, 10,\n",
    "                                    help=\"Contrôle le chevauchement entre les chunks pour préserver le contexte\")\n",
    "        \n",
    "        embedding_model = st.selectbox(\n",
    "            \"Modèle d'embedding\",\n",
    "            [\"text-embedding-3-small\", \"text-embedding-3-large\"],\n",
    "            index=0,\n",
    "            help=\"Modèle utilisé pour créer les embeddings vectoriels\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Fonction pour traiter les nouveaux fichiers\n",
    "    def process_uploaded_files(uploaded_files, vector_store):\n",
    "        if not uploaded_files:\n",
    "            return vector_store, 0\n",
    "        \n",
    "        # Créer un dossier temporaire pour les fichiers\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        \n",
    "        # Enregistrer les fichiers téléchargés\n",
    "        pdf_paths = []\n",
    "        for uploaded_file in uploaded_files:\n",
    "            file_path = os.path.join(temp_dir, uploaded_file.name)\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(uploaded_file.getbuffer())\n",
    "            pdf_paths.append(file_path)\n",
    "        \n",
    "        # Initialiser le text splitter pour un chunking plus fin\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        )\n",
    "        \n",
    "        # Traiter chaque fichier\n",
    "        new_chunks = []\n",
    "        for pdf_path in pdf_paths:\n",
    "            try:\n",
    "                # Utiliser PyMuPDF pour extraire texte et tableaux\n",
    "                doc = fitz.open(pdf_path)\n",
    "                pdf_text = \"\"\n",
    "                \n",
    "                doc_metadata = {\n",
    "                    \"source\": pdf_path,\n",
    "                    \"title\": os.path.basename(pdf_path),\n",
    "                    \"total_pages\": len(doc)\n",
    "                }\n",
    "                \n",
    "                for page_num, page in enumerate(doc):\n",
    "                    # Extraire le texte\n",
    "                    text = page.get_text(\"text\")\n",
    "                    \n",
    "                    # Extraire les tableaux en format texte structuré\n",
    "                    tables = page.get_text(\"blocks\")\n",
    "                    table_text = \"\"\n",
    "                    for block in tables:\n",
    "                        if len(block) > 6 and block[6] == 1:  # Type 1 = table\n",
    "                            table_text += f\"\\nTableau: {block[4]}\\n\"\n",
    "                    \n",
    "                    # Extraire et décrire les images\n",
    "                    image_info = \"\"\n",
    "                    for img in page.get_images(full=True):\n",
    "                        image_info += f\"\\n[Image: {img[0]}]\\n\"\n",
    "                    \n",
    "                    # Combiner avec des marqueurs de structure\n",
    "                    page_content = f\"# Page {page_num+1}\\n{text}\\n{table_text}\\n{image_info}\"\n",
    "                    pdf_text += page_content\n",
    "                \n",
    "                # Découper avec préservation de la structure\n",
    "                try:\n",
    "                    header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[\n",
    "                        (\"#\", \"header1\"),\n",
    "                        (\"##\", \"header2\"),\n",
    "                    ])\n",
    "                    md_header_splits = header_splitter.split_text(pdf_text)\n",
    "                    \n",
    "                    # Découper en chunks tout en préservant les métadonnées hiérarchiques\n",
    "                    chunks = []\n",
    "                    for split in md_header_splits:\n",
    "                        # Préserver le contexte hiérarchique dans les métadonnées\n",
    "                        split_metadata = {**doc_metadata, **split.metadata}\n",
    "                        # Découper davantage si nécessaire\n",
    "                        smaller_chunks = text_splitter.split_text(split.page_content)\n",
    "                        # Créer des Documents avec contexte préservé\n",
    "                        for chunk in smaller_chunks:\n",
    "                            chunks.append(Document(page_content=chunk, metadata=split_metadata))\n",
    "                except Exception as e:\n",
    "                    # Fallback si le découpage hiérarchique échoue\n",
    "                    chunks = []\n",
    "                    pages = [Document(page_content=page.get_text(\"text\"), \n",
    "                                     metadata={\"source\": pdf_path, \"page\": i+1}) \n",
    "                            for i, page in enumerate(doc)]\n",
    "                    chunks = text_splitter.split_documents(pages)\n",
    "                \n",
    "                new_chunks.extend(chunks)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Erreur avec {os.path.basename(pdf_path)}: {str(e)}\")\n",
    "        \n",
    "        # Créer les embeddings et ajouter à l'index\n",
    "        if new_chunks:\n",
    "            try:\n",
    "                # S'assurer que l'instance d'embeddings est disponible\n",
    "                embeddings = OpenAIEmbeddings(model=embedding_model)\n",
    "                \n",
    "                # Ajouter directement les documents à FAISS\n",
    "                vector_store.add_documents(new_chunks)\n",
    "                \n",
    "                # Sauvegarde de l'index FAISS mis à jour\n",
    "                vector_store.save_local(index_folder)\n",
    "                \n",
    "                # Nettoyer les fichiers temporaires\n",
    "                shutil.rmtree(temp_dir)\n",
    "                \n",
    "                return vector_store, len(new_chunks)\n",
    "            except Exception as e:\n",
    "                st.error(f\"Erreur lors de la mise à jour de l'index: {str(e)}\")\n",
    "        \n",
    "        # Nettoyer les fichiers temporaires\n",
    "        shutil.rmtree(temp_dir)\n",
    "        \n",
    "        return vector_store, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:21:37.048 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.048 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.051 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.052 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.054 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.056 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.057 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.057 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:37.059 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    # Bouton pour déclencher le traitement avec barre de progression\n",
    "    if uploaded_files:\n",
    "        if st.button(\"📥 Traiter et indexer les fichiers\", type=\"primary\", use_container_width=True):\n",
    "            with st.spinner(\"Traitement des fichiers en cours...\"):\n",
    "                progress_bar = st.progress(0)\n",
    "                total_files = len(uploaded_files)\n",
    "                \n",
    "                for i, _ in enumerate(uploaded_files):\n",
    "                    progress_bar.progress((i + 1) / total_files)\n",
    "                \n",
    "                vector_store, num_added = process_uploaded_files(uploaded_files, vector_store)\n",
    "                \n",
    "                if num_added > 0:\n",
    "                    st.success(f\"✅ {num_added} nouveaux chunks ont été ajoutés à l'index à partir de {len(uploaded_files)} fichiers!\")\n",
    "                    # Rafraîchir les données\n",
    "                    st.cache_data.clear()\n",
    "                    st.experimental_rerun()\n",
    "                else:\n",
    "                    st.error(\"❌ Aucun chunk n'a pu être ajouté. Vérifiez les fichiers et réessayez.\")\n",
    "    \n",
    "    # Afficher les statistiques sur l'index actuel\n",
    "    st.markdown('<div class=\"sub-header\">📊 Statistiques de l\\'index</div>', unsafe_allow_html=True)\n",
    "    \n",
    "    if success:\n",
    "        try:\n",
    "            # Récupérer les statistiques\n",
    "            all_docs = vector_store.docstore._dict.values()\n",
    "            total_chunks = len(vector_store.index_to_docstore_id)\n",
    "            \n",
    "            # Analyser les documents\n",
    "            sources_dict = {}\n",
    "            pages_dict = {}\n",
    "            \n",
    "            for doc in all_docs:\n",
    "                source = os.path.basename(doc.metadata.get('source', 'Inconnu'))\n",
    "                if source in sources_dict:\n",
    "                    sources_dict[source] += 1\n",
    "                else:\n",
    "                    sources_dict[source] = 1\n",
    "                \n",
    "                page = doc.metadata.get('page', 'N/A')\n",
    "                if source in pages_dict:\n",
    "                    if page not in pages_dict[source]:\n",
    "                        pages_dict[source].append(page)\n",
    "                else:\n",
    "                    pages_dict[source] = [page]\n",
    "            \n",
    "            # Afficher les statistiques générales\n",
    "            col1, col2, col3 = st.columns(3)\n",
    "            with col1:\n",
    "                st.markdown('<div class=\"metric-container\">', unsafe_allow_html=True)\n",
    "                st.markdown(f'<div class=\"metric-value\">{total_chunks}</div>', unsafe_allow_html=True)\n",
    "                st.markdown('<div class=\"metric-label\">Chunks totaux</div>', unsafe_allow_html=True)\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "                \n",
    "            with col2:\n",
    "                st.markdown('<div class=\"metric-container\">', unsafe_allow_html=True)\n",
    "                st.markdown(f'<div class=\"metric-value\">{len(sources_dict)}</div>', unsafe_allow_html=True)\n",
    "                st.markdown('<div class=\"metric-label\">Documents</div>', unsafe_allow_html=True)\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "                \n",
    "            with col3:\n",
    "                total_pages = sum(len(pages) for pages in pages_dict.values())\n",
    "                st.markdown('<div class=\"metric-container\">', unsafe_allow_html=True)\n",
    "                st.markdown(f'<div class=\"metric-value\">{total_pages}</div>', unsafe_allow_html=True)\n",
    "                st.markdown('<div class=\"metric-label\">Pages</div>', unsafe_allow_html=True)\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "            \n",
    "            # Visualiser la répartition des chunks\n",
    "            if sources_dict:\n",
    "                st.subheader(\"Répartition des chunks par document\")\n",
    "                \n",
    "                # Créer un dataframe pour la visualisation\n",
    "                df_sources = pd.DataFrame({\n",
    "                    'Document': list(sources_dict.keys()),\n",
    "                    'Nombre de chunks': list(sources_dict.values())\n",
    "                })\n",
    "                \n",
    "                # Trier par nombre de chunks\n",
    "                df_sources = df_sources.sort_values('Nombre de chunks', ascending=False)\n",
    "                \n",
    "                # Créer le graphique\n",
    "                fig, ax = plt.subplots(figsize=(10, 6))\n",
    "                sns.barplot(x='Document', y='Nombre de chunks', data=df_sources, palette='viridis', ax=ax)\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                st.pyplot(fig)\n",
    "                \n",
    "                # Afficher le tableau détaillé\n",
    "                st.subheader(\"Détails des documents indexés\")\n",
    "                \n",
    "                # Préparer les données pour le tableau\n",
    "                table_data = []\n",
    "                for source, chunks in sources_dict.items():\n",
    "                    pages = len(pages_dict.get(source, []))\n",
    "                    table_data.append({\n",
    "                        \"Document\": source,\n",
    "                        \"Nombre de chunks\": chunks,\n",
    "                        \"Nombre de pages\": pages,\n",
    "                        \"Chunks/page\": round(chunks / max(1, pages), 2)\n",
    "                    })\n",
    "                \n",
    "                # Afficher le tableau\n",
    "                st.markdown('<div class=\"dataframe-container\">', unsafe_allow_html=True)\n",
    "                st.dataframe(\n",
    "                    pd.DataFrame(table_data),\n",
    "                    use_container_width=True,\n",
    "                    hide_index=True\n",
    "                )\n",
    "                st.markdown('</div>', unsafe_allow_html=True)\n",
    "                \n",
    "        except Exception as e:\n",
    "            st.error(f\"Erreur lors de la récupération des statistiques: {str(e)}\")\n",
    "    else:\n",
    "        st.error(\"Index FAISS non chargé. Impossible d'afficher les statistiques.\")\n",
    "    \n",
    "    # Option pour réinitialiser l'index\n",
    "    with st.expander(\"⚠️ Réinitialiser l'index\"):\n",
    "        st.warning(\"Attention: Cette opération supprimera définitivement l'index FAISS actuel et toutes les données qu'il contient.\")\n",
    "        if st.button(\"🗑️ Réinitialiser l'index\", use_container_width=True):\n",
    "            confirm = st.text_input(\"Tapez 'CONFIRMER' pour réinitialiser l'index\", key=\"confirm_reset\")\n",
    "            if confirm == \"CONFIRMER\":\n",
    "                try:\n",
    "                    # Supprimer l'index\n",
    "                    if os.path.exists(os.path.join(index_folder, \"index.faiss\")):\n",
    "                        os.remove(os.path.join(index_folder, \"index.faiss\"))\n",
    "                    if os.path.exists(os.path.join(index_folder, \"index.pkl\")):\n",
    "                        os.remove(os.path.join(index_folder, \"index.pkl\"))\n",
    "                    \n",
    "                    st.success(\"✅ Index réinitialisé avec succès! L'application va redémarrer.\")\n",
    "                    st.cache_data.clear()\n",
    "                    st.cache_resource.clear()\n",
    "                    st.experimental_rerun()\n",
    "                except Exception as e:\n",
    "                    st.error(f\"Erreur lors de la réinitialisation: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 21:21:41.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.520 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.525 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.526 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.528 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.529 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.531 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-18 21:21:41.532 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# Pied de page\n",
    "st.markdown(\"---\")\n",
    "col1, col2 = st.columns([3, 1])\n",
    "with col1:\n",
    "    st.markdown(\"Développé avec RAG avancé et Streamlit\")\n",
    "with col2:\n",
    "    st.markdown(\"Version: 3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
